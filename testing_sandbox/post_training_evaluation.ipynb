{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post training with hybrid loss evaluation against a sanity check model\n",
    "The objective of this notebook is to demonstrate that one can optimise a network via training with a hybrid loss. \n",
    "The training losses (available on wandb at this link: ) demonstrate minimal performance improvement at asymptotic convergence than a sanity-check alternative. \n",
    "The objective here is to evaluate the quality of explanations which have been generated by these models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "The following functions contain the bulk of the specific helper functions necessary to evaluate the performance and explanation of VGG16 vs VGG19. \n",
    "Generic helper functions have been moved to the utils folder for use in other notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from baselines.trainVggBaselineForCIFAR10 import vgg\n",
    "from experiments import WrapperNet, perform_lrp_plain, evaluate_performance, evaluate_explanations\n",
    "from internal_utils import get_vgg16, get_vgg19, get_pretrained_model, get_CIFAR10_dataloader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TRUNCATE = 25\n",
    "print(f\"WARNING: TRUNCATING THE DATASET TO {TRUNCATE} --- THIS WILL ALSO BE TRUNCATED IN THE FUNCTIONS INCLUDED IN THE experiments/run_evaluation.py FILE\")\n",
    "print(f\"TO RUN OVER THE ENTIRE DATASET, UNCOMMENT THE RELEVANT LINES IN THE FUNCTIONS (SEARCH FOR THE STRING 'TRUNCATE')\")\n",
    "\n",
    "\n",
    "def plot_comparative_figure(df, method_0, method_1, data_type=\"Train\"):\n",
    "    \"\"\"\n",
    "    Plot a comparative figure of the results between the two models.\n",
    "    \"\"\"\n",
    "    figs_per_row = [\"distance_noise_small\", \"distance_noise_large\", \"distance_blur_small\", \"distance_blur_large\"]\n",
    "\n",
    "    # Create a single row figure with two boxplots per column\n",
    "    fig, axs = plt.subplots(1, len(figs_per_row), figsize=(20, 5), sharey=True)\n",
    "\n",
    "    for j, fig_type in enumerate(figs_per_row):\n",
    "        # Filter data for method_0\n",
    "        if \"small\" in fig_type:\n",
    "            df_method_0 = df[df[f\"{method_0}_{fig_type}_class_change\"] == False]\n",
    "        else:\n",
    "            df_method_0 = df[df[f\"{method_0}_{fig_type}_class_change\"] == True]\n",
    "        \n",
    "        # Filter data for method_1\n",
    "        if \"small\" in fig_type:\n",
    "            df_method_1 = df[df[f\"{method_1}_{fig_type}_class_change\"] == False]\n",
    "        else:\n",
    "            df_method_1 = df[df[f\"{method_1}_{fig_type}_class_change\"] == True]\n",
    "\n",
    "        # Combine the data for boxplot using a hue for methods\n",
    "        combined_df = pd.DataFrame({\n",
    "            'Value': pd.concat([df_method_0[f\"{method_0}_{fig_type}\"], df_method_1[f\"{method_1}_{fig_type}\"]]),\n",
    "            'Method': [method_0] * len(df_method_0) + [method_1] * len(df_method_1)\n",
    "        })\n",
    "\n",
    "        # Create boxplot\n",
    "        sns.boxplot(x='Method', y='Value', hue= 'Method', data=combined_df, ax=axs[j])\n",
    "        axs[j].set_title(f\"{fig_type}\".replace(\"_\", \" \"))\n",
    "    fig.suptitle(f\"Comparative Analysis of {method_0} and {method_1} on {data_type} Data\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training evaluation\n",
    "Here, we evaluate the performance of the models on the test and train data, and then evaluate the quality of explanations over the same datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "hybrid_model = get_pretrained_model(\"/Users/charleshiggins/Personal/CharlesPhD/CodeRepo/xai_intervention/RL-LRP/model_files/checkpoint_299_2024-08-06_02-23-55_default.tar\", vgg.vgg11)\n",
    "baseline_model  = get_pretrained_model(\"/Users/charleshiggins/Personal/CharlesPhD/CodeRepo/xai_intervention/RL-LRP/model_files/checkpoint_299_2024-08-06_11-16-09_sanity_check.tar\", vgg.vgg11)\n",
    "\n",
    "\n",
    "train_data, test_data = get_CIFAR10_dataloader(train=True, batch_size=8, num_workers=4, pin_memory=True), get_CIFAR10_dataloader(train=False, batch_size=8, num_workers=4, pin_memory=True)\n",
    "hybrid_results_train, hybrid_results_test = evaluate_performance(hybrid_model, train_data, test_data, convert_to_imagenet_labels=False)\n",
    "baseline_results_train, baseline_results_test  = evaluate_performance(baseline_model, train_data, test_data, convert_to_imagenet_labels=False)\n",
    "\n",
    "# visualise results from the initial dataframes\n",
    "hybrid_results_train['model'] = 'HYBRID'\n",
    "hybrid_results_test['model'] = 'HYBRID'\n",
    "baseline_results_train['model'] = 'BASELINE'\n",
    "baseline_results_test['model'] = 'BASELINE'\n",
    "\n",
    "# Concatenate dataframes\n",
    "combined_df_train = pd.concat([hybrid_results_train, baseline_results_train])\n",
    "combined_df_test = pd.concat([hybrid_results_test, baseline_results_test])\n",
    "\n",
    "# Melt the dataframe to long format for seaborn\n",
    "melted_df_train = combined_df_train.melt(id_vars=['model'], var_name='metric', value_name='value')\n",
    "melted_df_test = combined_df_test.melt(id_vars=['model'], var_name='metric', value_name='value')\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='metric', y='value', hue='model', data=melted_df_train)\n",
    "plt.title('Model Performance Comparison (Train Set)')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(title='Model (Train)')\n",
    "plt.show()\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='metric', y='value', hue='model', data=melted_df_test)\n",
    "plt.title('Model Performance Comparison (Test Set)')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(title='Model (Test)')\n",
    "plt.show()\n",
    "\n",
    "# To evaluate the explanations, we need to pass in a list of \"methods\"\n",
    "# each method is a tuple of the form (name, method, model)\n",
    "# name is a string which identifies the method -- i.e. \"VGG16\"\n",
    "# method is a function which generates a heatmap on a certain model --- i.e. perform_lrp_plain\n",
    "# model is the model which the method is applied to --- it needs to be in the heatmap form (i.e.)\n",
    "methods = [\n",
    "        (\"HYBRID\", perform_lrp_plain, WrapperNet(hybrid_model, hybrid_loss=True)),\n",
    "        (\"BASELINE\", perform_lrp_plain, WrapperNet(baseline_model, hybrid_loss=True))\n",
    "    ]\n",
    "# now evaluate explanations\n",
    "df_train, df_test = evaluate_explanations(train_data, test_data, methods, save_results = False, convert_to_imagenet_labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparative_figure(df_test, \"HYBRID\", \"BASELINE\", \"Test\")\n",
    "plot_comparative_figure(df_train, \"HYBRID\", \"BASELINE\", \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
