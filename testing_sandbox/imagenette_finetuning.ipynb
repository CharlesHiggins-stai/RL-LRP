{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LRP Fine tuning\n",
    "The objective of this notebook is to demonstrate that one can optimise a network post training. \n",
    "We demonstrate here that by incorporating attention information, we can improve the quality of models. \n",
    "This has several steps: \n",
    "- Download the Imagenette dataset (we use this rather than the full imagenet as it is considerably more lightweight)\n",
    "- Download two pre-trained models on Imagenet (from the VGG family)\n",
    "- Evaluate the quality of the models on the dataset. \n",
    "- Use the better performing model as the \"teacher model\" \n",
    "- Attempt to improve the learner model using the teacher's heatmaps. \n",
    "- Periodically evaluate performance\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "The following functions contain the bulk of the specific helper functions necessary to evaluate the performance and explanation of VGG16 vs VGG19. \n",
    "Generic helper functions have been moved to the utils folder for use in other notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import pandas as pd  \n",
    "from experiments import WrapperNet, perform_lrp_plain, evaluate_performance, evaluate_explanations\n",
    "# evaluate_performance, process_dataset, evaluate_explanations\n",
    "from internal_utils import get_data_imagenette, get_data_imagenette, get_vgg16, get_vgg19\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TRUNCATE = 25\n",
    "print(f\"WARNING: TRUNCATING THE DATASET TO {TRUNCATE} --- THIS WILL ALSO BE TRUNCATED IN THE FUNCTIONS INCLUDED IN THE experiments/run_evaluation.py FILE\")\n",
    "print(f\"TO RUN OVER THE ENTIRE DATASET, UNCOMMENT THE RELEVANT LINES IN THE FUNCTIONS (SEARCH FOR THE STRING 'TRUNCATE')\")\n",
    "\n",
    "\n",
    "def plot_comparative_figure(df, method_0, method_1, data_type=\"Train\"):\n",
    "    \"\"\"\n",
    "    Plot a comparative figure of the results between the two models.\n",
    "    \"\"\"\n",
    "    figs_per_row = [\"distance_noise_small\", \"distance_noise_large\", \"distance_blur_small\", \"distance_blur_large\"]\n",
    "\n",
    "    # Create a single row figure with two boxplots per column\n",
    "    fig, axs = plt.subplots(1, len(figs_per_row), figsize=(20, 5), sharey=True)\n",
    "\n",
    "    for j, fig_type in enumerate(figs_per_row):\n",
    "        # Filter data for method_0\n",
    "        if \"small\" in fig_type:\n",
    "            df_method_0 = df[df[f\"{method_0}_{fig_type}_class_change\"] == False]\n",
    "        else:\n",
    "            df_method_0 = df[df[f\"{method_0}_{fig_type}_class_change\"] == True]\n",
    "        \n",
    "        # Filter data for method_1\n",
    "        if \"small\" in fig_type:\n",
    "            df_method_1 = df[df[f\"{method_1}_{fig_type}_class_change\"] == False]\n",
    "        else:\n",
    "            df_method_1 = df[df[f\"{method_1}_{fig_type}_class_change\"] == True]\n",
    "\n",
    "        # Combine the data for boxplot using a hue for methods\n",
    "        combined_df = pd.DataFrame({\n",
    "            'Value': pd.concat([df_method_0[f\"{method_0}_{fig_type}\"], df_method_1[f\"{method_1}_{fig_type}\"]]),\n",
    "            'Method': [method_0] * len(df_method_0) + [method_1] * len(df_method_1)\n",
    "        })\n",
    "\n",
    "        # Create boxplot\n",
    "        sns.boxplot(x='Method', y='Value', hue= 'Method', data=combined_df, ax=axs[j])\n",
    "        axs[j].set_title(f\"{fig_type}\".replace(\"_\", \" \"))\n",
    "    fig.suptitle(f\"Comparative Analysis of {method_0} and {method_1} on {data_type} Data\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training evaluation\n",
    "Here, we evaluate the performance of the models on the test and train data, and then evaluate the quality of explanations over the same datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "train_data, test_data = get_data_imagenette()\n",
    "vgg16_results = evaluate_performance(get_vgg16(), train_data, test_data)\n",
    "vgg19_results = evaluate_performance(get_vgg19(), train_data, test_data)\n",
    "\n",
    "# visualise results from the initial dataframes\n",
    "vgg16_results['model'] = 'VGG16'\n",
    "vgg19_results['model'] = 'VGG19'\n",
    "\n",
    "# Concatenate dataframes\n",
    "combined_df = pd.concat([vgg16_results, vgg19_results])\n",
    "\n",
    "# Melt the dataframe to long format for seaborn\n",
    "melted_df = combined_df.melt(id_vars=['model'], var_name='metric', value_name='value')\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.boxplot(x='metric', y='value', hue='model', data=melted_df)\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(title='Model')\n",
    "plt.show()\n",
    "\n",
    "# To evaluate the explanations, we need to pass in a list of \"methods\"\n",
    "# each method is a tuple of the form (name, method, model)\n",
    "# name is a string which identifies the method -- i.e. \"VGG16\"\n",
    "# method is a function which generates a heatmap on a certain model --- i.e. perform_lrp_plain\n",
    "# model is the model which the method is applied to --- it needs to be in the heatmap form (i.e.)\n",
    "vgg16 = get_vgg16()\n",
    "vgg19 = get_vgg19()\n",
    "methods = [\n",
    "        (\"VGG16\", perform_lrp_plain, WrapperNet(vgg16, hybrid_loss=True)),\n",
    "        (\"VGG19\", perform_lrp_plain, WrapperNet(vgg19, hybrid_loss=True))\n",
    "    ]\n",
    "# now evaluate explanations\n",
    "df_train, df_test = evaluate_explanations(train_data, test_data, methods, save_results = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparative_figure(df_test, \"VGG16\", \"VGG19\", \"Test\")\n",
    "plot_comparative_figure(df_train, \"VGG16\", \"VGG19\", \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
