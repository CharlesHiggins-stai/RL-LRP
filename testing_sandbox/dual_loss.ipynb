{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('/Users/charleshiggins/Personal/CharlesPhD/CodeRepo/xai_intervention/RL-LRP/')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from experiments import SimpleRNet, apply_threshold, CosineDistanceLoss, ManualCNN\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "# comment out when running locally\n",
    "from experiments import WrapperNet\n",
    "# comment out when running locally            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_outer_model(wrapped_model, optimizer, criterion, train_loader, device, attention_function):\n",
    "    total_loss = 0\n",
    "    wrapped_model.train()\n",
    "    for _ in range(10):\n",
    "        data, target = next(iter(train_loader))\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        target_map = attention_function(data, threshold=0.95) # threshold is 0.99\n",
    "        optimizer.zero_grad()\n",
    "        output = wrapped_model(data, target)\n",
    "        loss = criterion(output, target_map)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def test_outer_model(wrapped_model, criterion, test_loader, device, attention_function):\n",
    "    wrapped_model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            target_map = attention_function(data, threshold=0.95)\n",
    "            output = wrapped_model(data, target)\n",
    "            test_loss += criterion(output, target_map).item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    return test_loss    \n",
    "\n",
    "def plot_heatmap_comparison(wrapped_model, test_loader, device, attention_function, epoch):\n",
    "    data, target = next(iter(test_loader))\n",
    "    target_map = attention_function(data, threshold=0.95)\n",
    "    output = wrapped_model(data.to(device), target.to(device))\n",
    "    num = np.random.randint(0, len(target))\n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    axes[0].imshow(output[num][0].detach().numpy(), cmap='hot')\n",
    "    axes[0].set_title(f'LRP Output after {epoch} iterations')\n",
    "    axes[1].imshow(target_map[num][0], cmap='hot')\n",
    "    axes[1].set_title('Original Image')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_inner_model(model_inner, optimizer, criterion, train_loader, device):\n",
    "    model_inner.train()\n",
    "    total_loss = 0\n",
    "    for _ in range(10):\n",
    "        data, target = next(iter(train_loader))\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model_inner(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss\n",
    "\n",
    "def test_inner_model(model_inner, criterion, test_loader, device):\n",
    "    model_inner.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model_inner(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            correct += output.argmax(dim=1).eq(target).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = (correct / len(test_loader.dataset)) * 100\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define device for GPU compatibility\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load and transform datasets\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the network and optimizer for the underlying network\n",
    "model_inner = SimpleRNet()\n",
    "optimizer_inner = optim.Adam(model_inner.parameters(), lr=1e-3)\n",
    "# now wrap the network in the LRP class\n",
    "model_outer = WrapperNet(SimpleRNet())\n",
    "optimizer_outer = optim.Adam(model_outer.parameters(), lr=1e-6)\n",
    "\n",
    "# define the loss functions for each\n",
    "criterion_outer = CosineDistanceLoss()\n",
    "criterion_inner = nn.CrossEntropyLoss()\n",
    "\n",
    "# Move to device\n",
    "model_inner.to(device)\n",
    "model_outer.to(device)\n",
    "\n",
    "EPOCHS = 100\n",
    "OUTER_TRAIN_FREQ = 1\n",
    "VISUALIZE_FREQ = 5\n",
    "for x in range(EPOCHS):\n",
    "    # syncronise parameters to get latest params from outer model\n",
    "    # so that we can train the inner model\n",
    "    model_inner.load_state_dict(model_outer.model.state_dict())\n",
    "    inner_train_loss  = train_inner_model(model_inner, optimizer_inner, criterion_inner, train_loader, device)\n",
    "    inner_test_loss, inner_accuracy = test_inner_model(model_inner, criterion_inner, test_loader, device)\n",
    "    print(f\"Epoch {x} Inner Test Loss: {inner_test_loss}; Inner Accuracy: {inner_accuracy}; Inner Train Loss: {inner_train_loss}\")\n",
    "    # make sure to then upload these into the outer model\n",
    "    # so that we can then train the outer model and see the changes tthat h\n",
    "    model_outer.model.load_state_dict(model_inner.state_dict())\n",
    "    if x % OUTER_TRAIN_FREQ == 0:\n",
    "        # belt and braces approach sync parameters again\n",
    "        model_outer.model.load_state_dict(model_inner.state_dict())\n",
    "        outer_train_loss = train_outer_model(model_outer, optimizer_outer, criterion_outer, train_loader, device, apply_threshold)\n",
    "        print(f'Epoch {x} Outer Train Session')\n",
    "        print(f'Epoch {x} Outer Train Loss: {outer_train_loss}')\n",
    "        inner_test_loss, inner_accuracy = test_inner_model(model_outer.model, criterion_inner, test_loader, device)\n",
    "        prev_test_loss, prev_inner_accuracy = test_inner_model(model_inner, criterion_inner, test_loader, device)\n",
    "        print(f\"Epoch {x} Inner Test Loss pre outer training: {prev_test_loss} Inner Accuracy pre outer training: {prev_inner_accuracy}\")\n",
    "        print(f\"Epoch {x} Inner Test Loss post outer training: {inner_test_loss} Inner Accuracy post outer training: {inner_accuracy}\")\n",
    "        print(f\"Epoch {x}: Inner Loss Delta {inner_test_loss - prev_test_loss}; Inner Accuracy Delta {inner_accuracy - prev_inner_accuracy}\")\n",
    "        # finally sync params at end of outer training loop\n",
    "        model_inner.load_state_dict(model_outer.model.state_dict())\n",
    "    if x % VISUALIZE_FREQ == 0:\n",
    "        plot_heatmap_comparison(model_outer, test_loader, device, apply_threshold, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minatar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
