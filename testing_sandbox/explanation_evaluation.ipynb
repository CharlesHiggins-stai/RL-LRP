{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet to prototype and visualize explanation metrics. \n",
    "\n",
    "There are several steps in this workflow. \n",
    "   - Function to compare sensitivity: in both, a small change should result in a small change, and a large change should result in a large change in the explanation.\n",
    "   - Random Noise\n",
    "   - Gaussian blur\n",
    "\n",
    "   \n",
    "   - Function to compare faithfullness (to be completed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def transform_batch_of_images(images):\n",
    "    \"\"\"Apply standard transformation to the batch of images.\"\"\"\n",
    "    # normalise the image to be in the right range\n",
    "    normalize_transform = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                        std=[0.229, 0.224, 0.225])\n",
    "    # convert image to tensor\n",
    "    to_tensor_transform = transforms.ToTensor()\n",
    "    return normalize_transform(to_tensor_transform(images))\n",
    "\n",
    "def get_data_imagenette(path = \"/Users/charleshiggins/Personal/CharlesPhD/CodeRepo/xai_intervention/RL-LRP/data/Imagenette\"):\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Load Imagenette dataset\n",
    "    imagenette_train = datasets.Imagenette(\n",
    "        root=path,  # Specify the directory to store the dataset\n",
    "        split='val',  # Use the validation split\n",
    "        transform=transform,\n",
    "        download=False  # Download the dataset if not already present\n",
    "    )\n",
    "\n",
    "    # Create a DataLoader\n",
    "    val_loader = DataLoader(\n",
    "        imagenette_train,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    return val_loader\n",
    "\n",
    "def get_data(path_to_data:str = '/home/charleshiggins/RL-LRP/baselines/trainVggBaselineForCIFAR10/data'):\n",
    "    \"\"\"Get Dataloader objects from Cifar10 dataset, with path passed in.\n",
    "\n",
    "    Args:\n",
    "        path_to_data (str): path to the data directory\n",
    "    N.B. Changed to Imagenette dataset from CIFAR210\n",
    "    Alternative here\n",
    "    datasets.CIFAR10(root=path_to_data, train=False, \n",
    "        batch_size=64, shuffle=False, num_workers=2, \n",
    "        pin_memory=True, transforms=transforms.ToTensor()\n",
    "    )\n",
    "    \"\"\"\n",
    "    val_loader = datasets.CIFAR10(root=path_to_data, train=False, \n",
    "        batch_size=64, shuffle=False, num_workers=2, \n",
    "        pin_memory=True, transforms=transforms.ToTensor()\n",
    "    )\n",
    "    return val_loader\n",
    "\n",
    "def blur_image_batch(images, kernel_size):\n",
    "    \"\"\"Blur the batch of images using a Gaussian kernel.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): batch of images to be blurred\n",
    "        kernel_size (int): size of the Gaussian kernel\n",
    "    Returns:\n",
    "        torch.Tensor: blurred images\n",
    "    \"\"\"\n",
    "    \n",
    "    blurred_images = torch.stack([TF.gaussian_blur(img, kernel_size=[kernel_size, kernel_size]) for img in images])\n",
    "    return blurred_images\n",
    "\n",
    "def add_random_noise_batch(images, noise_level):\n",
    "    \"\"\"Add random noise to the batch of images.\n",
    "\n",
    "    Args:\n",
    "        images (torch.Tensor): images to have noise added\n",
    "        noise_level (float): level of noise to be added\n",
    "    Returns:\n",
    "        torch.Tensor: images with noise added\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(images) * noise_level\n",
    "    noisy_images = images + noise\n",
    "    return noisy_images\n",
    "\n",
    "def compute_distance_between_images(images1, images2):\n",
    "    \"\"\"Compute the distance between two batches of images.\n",
    "\n",
    "    Args:\n",
    "        image1 (torch.Tensor): Tensor of treated images\n",
    "        image2 (torch.Tensor): Tensor of ground-truth images\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of distances between the two images\n",
    "    \"\"\"\n",
    "    # Flatten the images to compute cosine similarity\n",
    "    images1_flat = images1.view(images1.size(0), -1)\n",
    "    images2_flat = images2.view(images2.size(0), -1)\n",
    "    \n",
    "    # Compute cosine similarity and convert to cosine distance\n",
    "    cosine_similarity = F.cosine_similarity(images1_flat, images2_flat)\n",
    "    cosine_distance = 1 - cosine_similarity  # Convert similarity to distance\n",
    "    return cosine_distance\n",
    "\n",
    "\n",
    "def visulise_panel_image(image, model, kernel_size, noise_level):\n",
    "    \"\"\"Visualise the panel of images for the model.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): image to be visualised\n",
    "        model (torch.nn.Module): model to be visualised\n",
    "        kernel_size (int): size of the Gaussian kernel\n",
    "        noise_level (float): level of noise to be added\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "def visualise_panel_image(image, model, kernel_size, noise_level):\n",
    "    \"\"\"Visualise the panel of images for the model.\"\"\"\n",
    "    # Assume the image tensor is already in batch format, if not, unsqueeze it\n",
    "    if image.dim() == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    \n",
    "    original_image = image\n",
    "    # treated images\n",
    "    blurred_small = blur_image_batch(image, kernel_size_min)\n",
    "    blurred_large = blur_image_batch(image, kernel_size_max)\n",
    "    noisy_small = add_random_noise_batch(image, noise_level_min)\n",
    "    noisy_large = add_random_noise_batch(image, noise_level_max)\n",
    "    \n",
    "    # model outputs\n",
    "    original_heatmap = condense_to_heatmap(method(preprocess_images(image), label, model)).detach()\n",
    "    blurred_small_heatmap = condense_to_heatmap(method(preprocess_images(blurred_small), label, model)).detach()\n",
    "    blurred_large_heatmap = condense_to_heatmap(method(preprocess_images(blurred_large),label,  model)).detach()\n",
    "    noisy_small_heatmap = condense_to_heatmap( method(preprocess_images(noisy_small), label, model)).detach()\n",
    "    noisy_large_heatmap = condense_to_heatmap(method(preprocess_images(noisy_large), label, model)).detach()\n",
    "    \n",
    "    # Display images\n",
    "    fig, ax = plt.subplots(2, 5, figsize=(15, 5))\n",
    "    ax[0][0].imshow(original_image.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax[0][0].set_title('Original Image')\n",
    "    ax[0][1].imshow(blurred_small.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax[0][1].set_title('Small Blurred Image')\n",
    "    ax[0][2].imshow(blurred_large.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax[0][2].set_title('Large Blurred Image')\n",
    "    ax[0][3].imshow(noisy_small.squeeze().detach().permute(1, 2, 0).cpu().numpy())  # Example visualization\n",
    "    ax[0][3].set_title('Small Noisy Image')\n",
    "    ax[0][4].imshow(noisy_large.squeeze().detach().permute(1, 2, 0).cpu().numpy())  # Example visualization\n",
    "    ax[0][4].set_title('Large Noisy Image')\n",
    "    \n",
    "    ax[1][0].imshow(original_heatmap.squeeze(0), cmap='hot')\n",
    "    ax[1][0].set_title('Original Heatmap')\n",
    "    ax[1][1].imshow(blurred_small_heatmap.squeeze(0), cmap='hot')\n",
    "    ax[1][1].set_title('Small Blurred Heatmap')\n",
    "    ax[1][2].imshow(blurred_large_heatmap.squeeze(0), cmap='hot')\n",
    "    ax[1][2].set_title('Large Blurred Heatmap')\n",
    "    ax[1][3].imshow(noisy_small_heatmap.squeeze(0), cmap = 'hot')  # Example visualization\n",
    "    ax[1][3].set_title('Small Noisy Heatmap')\n",
    "    ax[1][4].imshow(noisy_large_heatmap.squeeze(0), cmap = 'hot')  # Example visualization\n",
    "    ax[1][4].set_title('Large Noisy Heatmap')\n",
    "    \n",
    "    for i in ax:\n",
    "        for j in i:\n",
    "            j.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# workflow for the visualisation and data analsysis\n",
    "import torch\n",
    "import pandas as pd\n",
    "# Load data\n",
    "# generate the blurred images, noisy images, and the ground truth heatmap images\n",
    "# then for each, calculate the distance between the heatmaps over the blurred images and the ground truth heatmap images\n",
    "def process_batch(\n",
    "    input_batch:torch.Tensor, \n",
    "    input_labels:torch.Tensor,  \n",
    "    methods: list, \n",
    "    kernel_size_min: float, \n",
    "    kernel_size_max:float, \n",
    "    noise_level_min: float, \n",
    "    noise_level_max: float):\n",
    "    \"\"\"Process the batch of images.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): model to be visualised\n",
    "        methods (list): list of methods(functions) to be used on each datapoint of form (name, method, model)\n",
    "        kernel_size (int): size of the Gaussian kernel\n",
    "        noise_level (float): level of noise to be added\n",
    "    Returns:\n",
    "        dict: dictionary of distances between the heatmaps\n",
    "    \"\"\"\n",
    "    results_dictionary = {}\n",
    "    for name, method, model in methods:\n",
    "        # get the ground truth heatmap using the method\n",
    "        ground_truth_heatmap = method(input_batch, input_labels, model)\n",
    "        # treat various images to get the noisy and blurred images\n",
    "        # run preprecoessing on the images --- normalise them to be within the right range\n",
    "        noisy_images_small = preprocess_images(add_random_noise_batch(input_batch, noise_level_min))\n",
    "        noisy_images_large = preprocess_images(add_random_noise_batch(input_batch, noise_level_max))\n",
    "        blurred_images_small = preprocess_images(blur_image_batch(input_batch, kernel_size_min))\n",
    "        blurred_images_large = preprocess_images(blur_image_batch(input_batch, kernel_size_max))\n",
    "        # generate the new heatmaps for each\n",
    "        noisy_heatmaps_small = method(noisy_images_small, input_labels, model)\n",
    "        noisy_heatmaps_large = method(noisy_images_large, input_labels, model)\n",
    "        blurred_heatmaps_small = method(blurred_images_small, input_labels, model)\n",
    "        blurred_heatmaps_large = method(blurred_images_large, input_labels, model)\n",
    "        # calculate the distance between the heatmaps\n",
    "        distance_noise_small = compute_distance_between_images(ground_truth_heatmap, noisy_heatmaps_small)\n",
    "        distance_noise_large = compute_distance_between_images(ground_truth_heatmap, noisy_heatmaps_large)\n",
    "        distance_blur_small = compute_distance_between_images(ground_truth_heatmap, blurred_heatmaps_small)\n",
    "        distance_blur_large = compute_distance_between_images(ground_truth_heatmap, blurred_heatmaps_large)\n",
    "        # calculate sparseness of heatmap\n",
    "        sparseness_original, sparseness_gini = compute_sparseness_of_heatmap(ground_truth_heatmap)\n",
    "        # store the results in the dictionary\n",
    "        results_dictionary[f\"{name}_distance_noise_small\"] = distance_noise_small\n",
    "        results_dictionary[f\"{name}_distance_noise_large\"] = distance_noise_large\n",
    "        results_dictionary[f\"{name}_distance_blur_small\"] = distance_blur_small\n",
    "        results_dictionary[f\"{name}_distance_blur_large\"] = distance_blur_large\n",
    "        results_dictionary[f\"{name}_sparseness_original\"] = sparseness_original\n",
    "        results_dictionary[f\"{name}_sparseness_gini\"] = sparseness_gini\n",
    "    # return data\n",
    "    return results_dictionary\n",
    "\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('/Users/charleshiggins/Personal/CharlesPhD/CodeRepo/xai_intervention/RL-LRP')\n",
    "from experiments import WrapperNet\n",
    "from captum.attr import GuidedGradCam\n",
    "\n",
    "def preprocess_images(image_batch):\n",
    "    \"\"\"Preprocess the image.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): image to be preprocessed\n",
    "    Returns:\n",
    "        torch.Tensor: preprocessed image\n",
    "    \"\"\"\n",
    "    if isinstance(image_batch, torch.Tensor) and image_batch.dim() == 4:\n",
    "        # normalise the images\n",
    "        normalize_transform = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                        std=[0.229, 0.224, 0.225])\n",
    "        images = normalize_transform(image_batch)\n",
    "        return images\n",
    "    else:\n",
    "        print(\"something went wrong in preprocessing images\")\n",
    "        print(f\"image batch is of shape {image_batch.shape}\")\n",
    "        raise ValueError(f\"Input must be a tensor of images -- unknown format {type(image_batch)} and dimension {image_batch.dim()}\")\n",
    "        \n",
    "\n",
    "def perform_lrp_plain(image, label, model):\n",
    "    \"\"\"Perform LRP on the image.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Tensor of images to be explained\n",
    "        labels (torch.Tensor): labels of the image (i.e. the class)\n",
    "        model (torch.nn.Module): model to be visualised\n",
    "    Returns:\n",
    "        torch.Tensor: heatmaps of the image\n",
    "    \"\"\"\n",
    "    assert isinstance(model, WrapperNet), \"Model must be a WrapperNet for LRP\"\n",
    "    class_idx, output = model(image, label)\n",
    "    return output\n",
    "\n",
    "def perform_loss_lrp(image, label, model):\n",
    "    \"\"\"Perform LRP on the image using the loss.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Tensor of images to be explained\n",
    "        labels (torch.Tensor): labels of the image (i.e. the class)\n",
    "        model (torch.nn.Module): model to be visualised\n",
    "    Returns:\n",
    "        torch.Tensor: heatmaps of the image\n",
    "    \"\"\"\n",
    "    assert isinstance(model, WrapperNet), \"Model must be a WrapperNet for LossLRP\"\n",
    "    class_idx, output = model(image, label)\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_input_output_layers(model):\n",
    "    \"\"\"\n",
    "    Gets the first and last convolutional layers of the model for GradCam\n",
    "    \n",
    "    Args:\n",
    "    - model: The PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "    - input_layer: The first convolutional layer\n",
    "    - output_layer: The last convolutional layer\n",
    "    \"\"\"\n",
    "    layers = list(model.modules())\n",
    "    conv_layers = [layer for layer in layers if isinstance(layer, torch.nn.Conv2d)]\n",
    "    \n",
    "    if not conv_layers:\n",
    "        raise ValueError(\"The model does not contain any Conv2d layers.\")\n",
    "    \n",
    "    input_layer = conv_layers[0]\n",
    "    output_layer = conv_layers[-1]\n",
    "    \n",
    "    return input_layer, output_layer\n",
    "\n",
    "def perform_gradcam(images, labels, model):\n",
    "    \"\"\"Perform GradCAM on the image.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Tensor of images to be explained\n",
    "        labels (torch.Tensor): labels of the image (i.e. the class)\n",
    "        model (torch.nn.Module): model to be visualised\n",
    "    Returns:\n",
    "        torch.Tensor: heatmaps of the image\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get the input and output layers\n",
    "    input_layer, output_layer = get_input_output_layers(model)\n",
    "    \n",
    "    # Create a LayerGradCam object\n",
    "    layer_gc = GuidedGradCam(model, output_layer)\n",
    "    \n",
    "    # Compute GradCAM attributions\n",
    "    attributions = layer_gc.attribute(images, target=labels)\n",
    "    \n",
    "    return attributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "def get_learner_model():\n",
    "    \"\"\"Get the learner model.\"\"\"\n",
    "    pass\n",
    "\n",
    "def get_teacher_model():\n",
    "    \"\"\" Load and return a pretrained VGG16 model from TorchVision\"\"\"\n",
    "    # Load the pretrained VGG16 model\n",
    "    model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Freeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move the model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # define params\n",
    "    kernel_size_min = 3\n",
    "    kernel_size_max = 5\n",
    "    noise_level_min = 0.1\n",
    "    noise_level_max = 0.2\n",
    "    # get the data\n",
    "    data_loader = get_data_imagenette()\n",
    "    # get the model\n",
    "    # learner_model = get_learner_model()\n",
    "    teacher_model = get_teacher_model()\n",
    "    # define the methods\n",
    "    methods = [\n",
    "        (\"LRP\", perform_lrp_plain, WrapperNet(teacher_model, hybrid_loss=True)),\n",
    "        # (\"LossLRP\", perform_loss_lrp, learner_model),\n",
    "        (\"GradCAM\", perform_gradcam, teacher_model),\n",
    "    ]\n",
    "    # process the data\n",
    "    table = {}\n",
    "    for i, (input_batch, input_labels) in tqdm(enumerate(data_loader)):\n",
    "        results = process_batch(\n",
    "            input_batch, \n",
    "            input_labels, \n",
    "            methods, \n",
    "            kernel_size_min, \n",
    "            kernel_size_max, \n",
    "            noise_level_min, \n",
    "            noise_level_max\n",
    "        )\n",
    "        # print the results\n",
    "        # print(f\"Batch {i} results: {results}\")\n",
    "        for key, value in results.items():\n",
    "            if key not in table.keys():\n",
    "                table[key] = value\n",
    "            else:\n",
    "                table[key] = torch.cat([table[key], value], dim = 0)\n",
    "    # convert to pandas dataframe\n",
    "    df = pd.DataFrame(table)\n",
    "    # save results\n",
    "    df.to_csv(\"test_results.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charleshiggins/Personal/CharlesPhD/CodeRepo/xai_intervention/RL-LRP/experiments/lrp_wrapper.py:120: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return torch.nn.functional.log_softmax(y), relevance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charleshiggins/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/captum/_utils/gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "/Users/charleshiggins/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/captum/attr/_core/guided_backprop_deconvnet.py:64: UserWarning: Setting backward hooks on ReLU activations.The hooks will be removed after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "LRP_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "LRP_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "LRP_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "LRP_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "LRP_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "LRP_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "LRP_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "LRP_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "LRP_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "LRP_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "LRP_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "LRP_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_noise_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_small\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_distance_blur_large\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_original\n",
      "<class 'torch.Tensor'>\n",
      "GradCAM_sparseness_gini\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m table \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (input_batch, input_labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[0;32m---> 44\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_size_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_size_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoise_level_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnoise_level_max\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# print the results\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# print(f\"Batch {i} results: {results}\")\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[6], line 37\u001b[0m, in \u001b[0;36mprocess_batch\u001b[0;34m(input_batch, input_labels, methods, kernel_size_min, kernel_size_max, noise_level_min, noise_level_max)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# generate the new heatmaps for each\u001b[39;00m\n\u001b[1;32m     36\u001b[0m noisy_heatmaps_small \u001b[38;5;241m=\u001b[39m method(noisy_images_small, input_labels, model)\n\u001b[0;32m---> 37\u001b[0m noisy_heatmaps_large \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_images_large\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m blurred_heatmaps_small \u001b[38;5;241m=\u001b[39m method(blurred_images_small, input_labels, model)\n\u001b[1;32m     39\u001b[0m blurred_heatmaps_large \u001b[38;5;241m=\u001b[39m method(blurred_images_large, input_labels, model)\n",
      "Cell \u001b[0;32mIn[7], line 98\u001b[0m, in \u001b[0;36mperform_gradcam\u001b[0;34m(images, labels, model)\u001b[0m\n\u001b[1;32m     95\u001b[0m layer_gc \u001b[38;5;241m=\u001b[39m GuidedGradCam(model, output_layer)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Compute GradCAM attributions\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m attributions \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_gc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attributions\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/captum/log/__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/captum/attr/_core/guided_grad_cam.py:183\u001b[0m, in \u001b[0;36mGuidedGradCam.attribute\u001b[0;34m(self, inputs, target, additional_forward_args, interpolate_mode, attribute_to_layer_input)\u001b[0m\n\u001b[1;32m    181\u001b[0m is_inputs_tuple \u001b[38;5;241m=\u001b[39m _is_tuple(inputs)\n\u001b[1;32m    182\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _format_tensor_into_tuples(inputs)\n\u001b[0;32m--> 183\u001b[0m grad_cam_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_cam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_cam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# self\u001b[39;49;00m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelu_attributions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(grad_cam_attr, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(grad_cam_attr) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGuidedGradCAM attributions for layer with multiple inputs / \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/captum/attr/_core/layer/grad_cam.py:195\u001b[0m, in \u001b[0;36mLayerGradCam.attribute\u001b[0;34m(self, inputs, target, additional_forward_args, attribute_to_layer_input, relu_attributions, attr_dim_summation)\u001b[0m\n\u001b[1;32m    190\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(\n\u001b[1;32m    191\u001b[0m     additional_forward_args\n\u001b[1;32m    192\u001b[0m )\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Returns gradient of output with respect to\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# hidden layer and hidden layer evaluated at each input.\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m layer_gradients, layer_evals \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_layer_gradients_and_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m summed_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    206\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmean(\n\u001b[1;32m    207\u001b[0m         layer_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer_grad \u001b[38;5;129;01min\u001b[39;00m layer_gradients\n\u001b[1;32m    214\u001b[0m )\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr_dim_summation:\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/captum/_utils/gradient.py:592\u001b[0m, in \u001b[0;36mcompute_layer_gradients_and_eval\u001b[0;34m(forward_fn, layer, inputs, target_ind, additional_forward_args, gradient_neuron_selector, device_ids, attribute_to_layer_input, output_fn)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03mComputes gradients of the output with respect to a given layer as well\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03mas the output evaluation of the layer for an arbitrary forward function\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;124;03m        Target layer output for given input.\u001b[39;00m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;66;03m# saved_layer is a dictionary mapping device to a tuple of\u001b[39;00m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;66;03m# layer evaluations on that device.\u001b[39;00m\n\u001b[0;32m--> 592\u001b[0m     saved_layer, output \u001b[38;5;241m=\u001b[39m \u001b[43m_forward_layer_distributed_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute_to_layer_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforward_hook_with_return\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequire_layer_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget not provided when necessary, cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m take gradient with respect to multiple outputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    607\u001b[0m     device_ids \u001b[38;5;241m=\u001b[39m _extract_device_ids(forward_fn, saved_layer, device_ids)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/captum/_utils/gradient.py:294\u001b[0m, in \u001b[0;36m_forward_layer_distributed_eval\u001b[0;34m(forward_fn, inputs, layer, target_ind, additional_forward_args, attribute_to_layer_input, forward_hook_with_return, require_layer_grads)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m             all_hooks\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    292\u001b[0m                 single_layer\u001b[38;5;241m.\u001b[39mregister_forward_hook(hook_wrapper(single_layer))\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m all_hooks:\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/captum/_utils/common.py:531\u001b[0m, in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    528\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _format_inputs(inputs)\n\u001b[1;32m    529\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[0;32m--> 531\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _select_targets(output, target)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/torchvision/models/vgg.py:66\u001b[0m, in \u001b[0;36mVGG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 66\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/torch/nn/modules/module.py:1582\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1580\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1582\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1584\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1586\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1587\u001b[0m     ):\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_lrp/lib/python3.12/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minatar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
