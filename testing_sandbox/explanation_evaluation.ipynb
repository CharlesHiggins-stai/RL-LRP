{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet to prototype and visualize explanation metrics. \n",
    "\n",
    "There are several steps in this workflow. \n",
    "   - Function to compare sensitivity: in both, a small change should result in a small change, and a large change should result in a large change in the explanation.\n",
    "   - Random Noise\n",
    "   - Gaussian blur\n",
    "\n",
    "   \n",
    "   - Function to compare faithfullness (to be completed)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def transform_batch_of_images(images):\n",
    "    \"\"\"Apply standard transformation to the batch of images.\"\"\"\n",
    "    # normalise the image to be in the right range\n",
    "    normalize_transform = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                        std=[0.229, 0.224, 0.225])\n",
    "    # convert image to tensor\n",
    "    to_tensor_transform = transforms.ToTensor()\n",
    "    return normalize_transform(to_tensor_transform(images))\n",
    "\n",
    "def get_data_imagenette(path = \"/Users/charleshiggins/Personal/CharlesPhD/CodeRepo/xai_intervention/RL-LRP/data/Imagenette\"):\n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Load Imagenette dataset\n",
    "    imagenette_train = datasets.Imagenette(\n",
    "        root=path,  # Specify the directory to store the dataset\n",
    "        split='val',  # Use the validation split\n",
    "        transform=transform,\n",
    "        download=False  # Download the dataset if not already present\n",
    "    )\n",
    "\n",
    "    # Create a DataLoader\n",
    "    val_loader = DataLoader(\n",
    "        imagenette_train,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    return val_loader\n",
    "\n",
    "def get_data(path_to_data:str = '/home/charleshiggins/RL-LRP/baselines/trainVggBaselineForCIFAR10/data'):\n",
    "    \"\"\"Get Dataloader objects from Cifar10 dataset, with path passed in.\n",
    "\n",
    "    Args:\n",
    "        path_to_data (str): path to the data directory\n",
    "    N.B. Changed to Imagenette dataset from CIFAR210\n",
    "    Alternative here\n",
    "    datasets.CIFAR10(root=path_to_data, train=False, \n",
    "        batch_size=64, shuffle=False, num_workers=2, \n",
    "        pin_memory=True, transforms=transforms.ToTensor()\n",
    "    )\n",
    "    \"\"\"\n",
    "    val_loader = datasets.CIFAR10(root=path_to_data, train=False, \n",
    "        batch_size=64, shuffle=False, num_workers=2, \n",
    "        pin_memory=True, transforms=transforms.ToTensor()\n",
    "    )\n",
    "    return val_loader\n",
    "\n",
    "def blur_image_batch(images, kernel_size):\n",
    "    \"\"\"Blur the batch of images using a Gaussian kernel.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): batch of images to be blurred\n",
    "        kernel_size (int): size of the Gaussian kernel\n",
    "    Returns:\n",
    "        torch.Tensor: blurred images\n",
    "    \"\"\"\n",
    "    \n",
    "    blurred_images = torch.stack([TF.gaussian_blur(img, kernel_size=[kernel_size, kernel_size]) for img in images])\n",
    "    return blurred_images\n",
    "\n",
    "def add_random_noise_batch(images, noise_level):\n",
    "    \"\"\"Add random noise to the batch of images.\n",
    "\n",
    "    Args:\n",
    "        images (torch.Tensor): images to have noise added\n",
    "        noise_level (float): level of noise to be added\n",
    "    Returns:\n",
    "        torch.Tensor: images with noise added\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(images) * noise_level\n",
    "    noisy_images = images + noise\n",
    "    return noisy_images\n",
    "\n",
    "def compute_distance_between_images(images1, images2):\n",
    "    \"\"\"Compute the distance between two batches of images.\n",
    "\n",
    "    Args:\n",
    "        image1 (torch.Tensor): Tensor of treated images\n",
    "        image2 (torch.Tensor): Tensor of ground-truth images\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of distances between the two images\n",
    "    \"\"\"\n",
    "    # Flatten the images to compute cosine similarity\n",
    "    images1_flat = images1.view(images1.size(0), -1)\n",
    "    images2_flat = images2.view(images2.size(0), -1)\n",
    "    \n",
    "    # Compute cosine similarity and convert to cosine distance\n",
    "    cosine_similarity = F.cosine_similarity(images1_flat, images2_flat)\n",
    "    cosine_distance = 1 - cosine_similarity  # Convert similarity to distance\n",
    "    return cosine_distance\n",
    "\n",
    "\n",
    "\n",
    "def visualise_panel_image(image, model, kernel_size, noise_level):\n",
    "    \"\"\"Visualise the panel of images for the model.\"\"\"\n",
    "    # Assume the image tensor is already in batch format, if not, unsqueeze it\n",
    "    if image.dim() == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    \n",
    "    original_image = image\n",
    "    # treated images\n",
    "    blurred_small = blur_image_batch(image, kernel_size_min)\n",
    "    blurred_large = blur_image_batch(image, kernel_size_max)\n",
    "    noisy_small = add_random_noise_batch(image, noise_level_min)\n",
    "    noisy_large = add_random_noise_batch(image, noise_level_max)\n",
    "    \n",
    "    # model outputs\n",
    "    original_heatmap = condense_to_heatmap(method(preprocess_images(image), label, model)).detach()\n",
    "    blurred_small_heatmap = condense_to_heatmap(method(preprocess_images(blurred_small), label, model)).detach()\n",
    "    blurred_large_heatmap = condense_to_heatmap(method(preprocess_images(blurred_large),label,  model)).detach()\n",
    "    noisy_small_heatmap = condense_to_heatmap( method(preprocess_images(noisy_small), label, model)).detach()\n",
    "    noisy_large_heatmap = condense_to_heatmap(method(preprocess_images(noisy_large), label, model)).detach()\n",
    "    \n",
    "    # Display images\n",
    "    fig, ax = plt.subplots(2, 5, figsize=(15, 5))\n",
    "    ax[0][0].imshow(original_image.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax[0][0].set_title('Original Image')\n",
    "    ax[0][1].imshow(blurred_small.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax[0][1].set_title('Small Blurred Image')\n",
    "    ax[0][2].imshow(blurred_large.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax[0][2].set_title('Large Blurred Image')\n",
    "    ax[0][3].imshow(noisy_small.squeeze().detach().permute(1, 2, 0).cpu().numpy())  # Example visualization\n",
    "    ax[0][3].set_title('Small Noisy Image')\n",
    "    ax[0][4].imshow(noisy_large.squeeze().detach().permute(1, 2, 0).cpu().numpy())  # Example visualization\n",
    "    ax[0][4].set_title('Large Noisy Image')\n",
    "    \n",
    "    ax[1][0].imshow(original_heatmap.squeeze(0), cmap='hot')\n",
    "    ax[1][0].set_title('Original Heatmap')\n",
    "    ax[1][1].imshow(blurred_small_heatmap.squeeze(0), cmap='hot')\n",
    "    ax[1][1].set_title('Small Blurred Heatmap')\n",
    "    ax[1][2].imshow(blurred_large_heatmap.squeeze(0), cmap='hot')\n",
    "    ax[1][2].set_title('Large Blurred Heatmap')\n",
    "    ax[1][3].imshow(noisy_small_heatmap.squeeze(0), cmap = 'hot')  # Example visualization\n",
    "    ax[1][3].set_title('Small Noisy Heatmap')\n",
    "    ax[1][4].imshow(noisy_large_heatmap.squeeze(0), cmap = 'hot')  # Example visualization\n",
    "    ax[1][4].set_title('Large Noisy Heatmap')\n",
    "    \n",
    "    for i in ax:\n",
    "        for j in i:\n",
    "            j.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow for the visualisation and data analsysis\n",
    "import torch\n",
    "import pandas as pd\n",
    "# Load data\n",
    "# generate the blurred images, noisy images, and the ground truth heatmap images\n",
    "# then for each, calculate the distance between the heatmaps over the blurred images and the ground truth heatmap images\n",
    "def process_batch(\n",
    "    input_batch:torch.Tensor, \n",
    "    input_labels:torch.Tensor,  \n",
    "    methods: list, \n",
    "    kernel_size_min: float, \n",
    "    kernel_size_max:float, \n",
    "    noise_level_min: float, \n",
    "    noise_level_max: float):\n",
    "    \"\"\"Process the batch of images.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): model to be visualised\n",
    "        methods (list): list of methods(functions) to be used on each datapoint of form (name, method, model)\n",
    "        kernel_size (int): size of the Gaussian kernel\n",
    "        noise_level (float): level of noise to be added\n",
    "    Returns:\n",
    "        dict: dictionary of distances between the heatmaps\n",
    "    \"\"\"\n",
    "    results_dictionary = {}\n",
    "    for name, method, model in methods:\n",
    "        # get the ground truth heatmap using the method\n",
    "        ground_truth_heatmap = method(input_batch, input_labels, model)\n",
    "        # treat various images to get the noisy and blurred images\n",
    "        # run preprecoessing on the images --- normalise them to be within the right range\n",
    "        noisy_images_small = preprocess_images(add_random_noise_batch(input_batch, noise_level_min))\n",
    "        noisy_images_large = preprocess_images(add_random_noise_batch(input_batch, noise_level_max))\n",
    "        blurred_images_small = preprocess_images(blur_image_batch(input_batch, kernel_size_min))\n",
    "        blurred_images_large = preprocess_images(blur_image_batch(input_batch, kernel_size_max))\n",
    "        # generate the new heatmaps for each\n",
    "        noisy_heatmaps_small = method(noisy_images_small, input_labels, model)\n",
    "        noisy_heatmaps_large = method(noisy_images_large, input_labels, model)\n",
    "        blurred_heatmaps_small = method(blurred_images_small, input_labels, model)\n",
    "        blurred_heatmaps_large = method(blurred_images_large, input_labels, model)\n",
    "        # calculate the distance between the heatmaps\n",
    "        distance_noise_small = compute_distance_between_images(ground_truth_heatmap, noisy_heatmaps_small)\n",
    "        distance_noise_large = compute_distance_between_images(ground_truth_heatmap, noisy_heatmaps_large)\n",
    "        distance_blur_small = compute_distance_between_images(ground_truth_heatmap, blurred_heatmaps_small)\n",
    "        distance_blur_large = compute_distance_between_images(ground_truth_heatmap, blurred_heatmaps_large)\n",
    "        # calculate sparseness of heatmap\n",
    "        sparseness_original, sparseness_gini = compute_sparseness_of_heatmap(ground_truth_heatmap)\n",
    "        # store the results in the dictionary\n",
    "        results_dictionary[f\"{name}_distance_noise_small\"] = distance_noise_small\n",
    "        results_dictionary[f\"{name}_distance_noise_large\"] = distance_noise_large\n",
    "        results_dictionary[f\"{name}_distance_blur_small\"] = distance_blur_small\n",
    "        results_dictionary[f\"{name}_distance_blur_large\"] = distance_blur_large\n",
    "        results_dictionary[f\"{name}_sparseness_original\"] = sparseness_original\n",
    "        results_dictionary[f\"{name}_sparseness_gini\"] = sparseness_gini\n",
    "    # return data\n",
    "    return results_dictionary\n",
    "\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append('/Users/charleshiggins/Personal/CharlesPhD/CodeRepo/xai_intervention/RL-LRP')\n",
    "from experiments import WrapperNet\n",
    "from captum.attr import GuidedGradCam\n",
    "\n",
    "def preprocess_images(image_batch):\n",
    "    \"\"\"Preprocess the image.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): image to be preprocessed\n",
    "    Returns:\n",
    "        torch.Tensor: preprocessed image\n",
    "    \"\"\"\n",
    "    if isinstance(image_batch, torch.Tensor) and image_batch.dim() == 4:\n",
    "        # normalise the images\n",
    "        normalize_transform = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                        std=[0.229, 0.224, 0.225])\n",
    "        images = normalize_transform(image_batch)\n",
    "        return images\n",
    "    else:\n",
    "        print(\"something went wrong in preprocessing images\")\n",
    "        print(f\"image batch is of shape {image_batch.shape}\")\n",
    "        raise ValueError(f\"Input must be a tensor of images -- unknown format {type(image_batch)} and dimension {image_batch.dim()}\")\n",
    "        \n",
    "\n",
    "def perform_lrp_plain(image, label, model):\n",
    "    \"\"\"Perform LRP on the image.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Tensor of images to be explained\n",
    "        labels (torch.Tensor): labels of the image (i.e. the class)\n",
    "        model (torch.nn.Module): model to be visualised\n",
    "    Returns:\n",
    "        torch.Tensor: heatmaps of the image\n",
    "    \"\"\"\n",
    "    assert isinstance(model, WrapperNet), \"Model must be a WrapperNet for LRP\"\n",
    "    class_idx, output = model(image, label)\n",
    "    return output\n",
    "\n",
    "def perform_loss_lrp(image, label, model):\n",
    "    \"\"\"Perform LRP on the image using the loss.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Tensor of images to be explained\n",
    "        labels (torch.Tensor): labels of the image (i.e. the class)\n",
    "        model (torch.nn.Module): model to be visualised\n",
    "    Returns:\n",
    "        torch.Tensor: heatmaps of the image\n",
    "    \"\"\"\n",
    "    assert isinstance(model, WrapperNet), \"Model must be a WrapperNet for LossLRP\"\n",
    "    class_idx, output = model(image, label)\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_input_output_layers(model):\n",
    "    \"\"\"\n",
    "    Gets the first and last convolutional layers of the model for GradCam\n",
    "    \n",
    "    Args:\n",
    "    - model: The PyTorch model\n",
    "    \n",
    "    Returns:\n",
    "    - input_layer: The first convolutional layer\n",
    "    - output_layer: The last convolutional layer\n",
    "    \"\"\"\n",
    "    layers = list(model.modules())\n",
    "    conv_layers = [layer for layer in layers if isinstance(layer, torch.nn.Conv2d)]\n",
    "    \n",
    "    if not conv_layers:\n",
    "        raise ValueError(\"The model does not contain any Conv2d layers.\")\n",
    "    \n",
    "    input_layer = conv_layers[0]\n",
    "    output_layer = conv_layers[-1]\n",
    "    \n",
    "    return input_layer, output_layer\n",
    "\n",
    "def perform_gradcam(images, labels, model):\n",
    "    \"\"\"Perform GradCAM on the image.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Tensor of images to be explained\n",
    "        labels (torch.Tensor): labels of the image (i.e. the class)\n",
    "        model (torch.nn.Module): model to be visualised\n",
    "    Returns:\n",
    "        torch.Tensor: heatmaps of the image\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get the input and output layers\n",
    "    input_layer, output_layer = get_input_output_layers(model)\n",
    "    \n",
    "    # Create a LayerGradCam object\n",
    "    layer_gc = GuidedGradCam(model, output_layer)\n",
    "    \n",
    "    # Compute GradCAM attributions\n",
    "    attributions = layer_gc.attribute(images, target=labels)\n",
    "    \n",
    "    return attributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "def get_learner_model():\n",
    "    \"\"\"Get the learner model.\"\"\"\n",
    "    pass\n",
    "\n",
    "def get_teacher_model():\n",
    "    \"\"\" Load and return a pretrained VGG16 model from TorchVision\"\"\"\n",
    "    # Load the pretrained VGG16 model\n",
    "    model = torchvision.models.vgg16(weights=torchvision.models.VGG16_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Freeze all parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move the model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # define params\n",
    "    kernel_size_min = 3\n",
    "    kernel_size_max = 5\n",
    "    noise_level_min = 0.1\n",
    "    noise_level_max = 0.2\n",
    "    # get the data\n",
    "    data_loader = get_data_imagenette()\n",
    "    # get the model\n",
    "    # learner_model = get_learner_model()\n",
    "    teacher_model = get_teacher_model()\n",
    "    # define the methods\n",
    "    methods = [\n",
    "        (\"LRP\", perform_lrp_plain, WrapperNet(teacher_model, hybrid_loss=True)),\n",
    "        # (\"LossLRP\", perform_loss_lrp, learner_model),\n",
    "        (\"GradCAM\", perform_gradcam, teacher_model),\n",
    "    ]\n",
    "    # process the data\n",
    "    table = {}\n",
    "    for i, (input_batch, input_labels) in tqdm(enumerate(data_loader)):\n",
    "        results = process_batch(\n",
    "            input_batch, \n",
    "            input_labels, \n",
    "            methods, \n",
    "            kernel_size_min, \n",
    "            kernel_size_max, \n",
    "            noise_level_min, \n",
    "            noise_level_max\n",
    "        )\n",
    "        # print the results\n",
    "        # print(f\"Batch {i} results: {results}\")\n",
    "        for key, value in results.items():\n",
    "            if key not in table.keys():\n",
    "                table[key] = value\n",
    "            else:\n",
    "                table[key] = torch.cat([table[key], value], dim = 0)\n",
    "    # convert to pandas dataframe\n",
    "    df = pd.DataFrame(table)\n",
    "    # save results\n",
    "    df.to_csv(\"test_results.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minatar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
