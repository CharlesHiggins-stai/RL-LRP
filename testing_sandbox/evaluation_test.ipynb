{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from experiments import perform_gradcam, perform_lrp_captum\n",
    "from internal_utils import preprocess_images, condense_to_heatmap, blur_image_batch, add_random_noise_batch, get_data_imagenette, get_teacher_model\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def visualise_panel_image(image, model, kernel_size_min, kernel_size_max, noise_level_min, noise_level_max, method, label):\n",
    "    \"\"\"Visualise the panel of images for the model.\"\"\"\n",
    "    # Assume the image tensor is already in batch format, if not, unsqueeze it\n",
    "    if image.dim() == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    \n",
    "    original_image = image\n",
    "    # treated images\n",
    "    blurred_small = blur_image_batch(image, kernel_size_min)\n",
    "    blurred_large = blur_image_batch(image, kernel_size_max)\n",
    "    noisy_small = add_random_noise_batch(image, noise_level_min)\n",
    "    noisy_large = add_random_noise_batch(image, noise_level_max)\n",
    "    \n",
    "    # model outputs\n",
    "    original_heatmap = condense_to_heatmap(method(preprocess_images(image), label, model)).detach()\n",
    "    blurred_small_heatmap = condense_to_heatmap(method(preprocess_images(blurred_small), label, model)).detach()\n",
    "    blurred_large_heatmap = condense_to_heatmap(method(preprocess_images(blurred_large),label,  model)).detach()\n",
    "    noisy_small_heatmap = condense_to_heatmap( method(preprocess_images(noisy_small), label, model)).detach()\n",
    "    noisy_large_heatmap = condense_to_heatmap(method(preprocess_images(noisy_large), label, model)).detach()\n",
    "    \n",
    "    # Display images\n",
    "    fig, ax = plt.subplots(2, 5, figsize=(15, 5))\n",
    "    ax[0][0].imshow(original_image.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax[0][0].set_title('Original Image')\n",
    "    ax[0][1].imshow(blurred_small.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax[0][1].set_title('Small Blurred Image')\n",
    "    ax[0][2].imshow(blurred_large.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax[0][2].set_title('Large Blurred Image')\n",
    "    ax[0][3].imshow(noisy_small.squeeze().detach().permute(1, 2, 0).cpu().numpy())  # Example visualization\n",
    "    ax[0][3].set_title('Small Noisy Image')\n",
    "    ax[0][4].imshow(noisy_large.squeeze().detach().permute(1, 2, 0).cpu().numpy())  # Example visualization\n",
    "    ax[0][4].set_title('Large Noisy Image')\n",
    "    \n",
    "    ax[1][0].imshow(original_heatmap.squeeze(0), cmap='seismic')\n",
    "    ax[1][0].set_title('Original Heatmap')\n",
    "    ax[1][1].imshow(blurred_small_heatmap.squeeze(0), cmap='seismic')\n",
    "    ax[1][1].set_title('Small Blurred Heatmap')\n",
    "    ax[1][2].imshow(blurred_large_heatmap.squeeze(0), cmap='seismic')\n",
    "    ax[1][2].set_title('Large Blurred Heatmap')\n",
    "    ax[1][3].imshow(noisy_small_heatmap.squeeze(0), cmap ='seismic')  # Example visualization\n",
    "    ax[1][3].set_title('Small Noisy Heatmap')\n",
    "    ax[1][4].imshow(noisy_large_heatmap.squeeze(0), cmap ='seismic')  # Example visualization\n",
    "    ax[1][4].set_title('Large Noisy Heatmap')\n",
    "    \n",
    "    for i in ax:\n",
    "        for j in i:\n",
    "            j.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data_imagenette()\n",
    "input_images, labels = next(iter(data))\n",
    "model = get_teacher_model()\n",
    "# define params\n",
    "kernel_size_min = 1\n",
    "kernel_size_max = 7\n",
    "noise_level_min = 0.1\n",
    "noise_level_max = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image, label = input_images[8], labels[8]\n",
    "\n",
    "visualise_panel_image(image, model, kernel_size_min, kernel_size_max, noise_level_min, noise_level_max, perform_gradcam, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_panel_image(image, model, kernel_size_min, kernel_size_max, noise_level_min, noise_level_max, perform_lrp_captum, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from experiments import perform_lrp_plain, WrapperNet\n",
    "visualise_panel_image(image, WrapperNet(model, hybrid_loss=True), kernel_size_min, kernel_size_max, noise_level_min, noise_level_max, perform_lrp_plain, label.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint = torch.load(\"/Users/charleshiggins/Personal/CharlesPhD/CodeRepo/xai_intervention/RL-LRP/data/trained_CIFAR_models/checkpoint_299_2024-07-21_17-27-55.tar\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import baselines.trainVggBaselineForCIFAR10.vgg as vgg\n",
    "model = WrapperNet(vgg.__dict__['vgg11'](), hybrid_loss=True)\n",
    "from internal_utils import update_dictionary_patch\n",
    "checkpoint = update_dictionary_patch(checkpoint)\n",
    "model.load_state_dict(checkpoint['new_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from internal_utils import get_CIFAR10_dataloader\n",
    "data_loader = get_CIFAR10_dataloader(train=False)\n",
    "data, target = next(iter(data_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = data[10], target[10]\n",
    "visualise_panel_image(image, model, kernel_size_min, kernel_size_max, noise_level_min, noise_level_max, perform_lrp_plain, label.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_panel_image(image, model.model, kernel_size_min, kernel_size_max, noise_level_min, noise_level_max, perform_lrp_captum, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
