{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 20, kernel_size=5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(320, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 320)  # Flatten the output for the classifier\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class DiffLrpWrapper(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        assert isinstance(net, nn.Module), f\"Expected net to be an instance of nn.Module, got {type(net)}\"\n",
    "        self.net = net\n",
    "        self.activations = {}\n",
    "        self.outputs = {}\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        # Register a forward hook on each module\n",
    "        for name, module in self.net.named_modules():\n",
    "            # Avoid registering hooks on containers\n",
    "            if len(list(module.children())) == 0:\n",
    "                module.register_forward_hook(self._save_activation(name))\n",
    "\n",
    "    def _save_activation(self, name):\n",
    "        # This method returns a hook function\n",
    "        def hook(module, input, output):\n",
    "            self.activations[name] = input[0].detach()\n",
    "            self.outputs[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x, target_class:torch.Tensor):\n",
    "        assert x.shape[0] == target_class.shape[0], f\"Expected x and target_class to have the same batch size, got {x.shape[0]} and {target_class.shape[0]}\"\n",
    "        # Forward pass through the network\n",
    "        initial_out = self.net(x)\n",
    "        # Create a mask that zeros out all elements except for the target class\n",
    "        mask = torch.zeros_like(initial_out)\n",
    "        mask[torch.arange(mask.size(0)), target_class] = 1  # Ensure target_class is either a scalar or has the same batch size as x\n",
    "\n",
    "        # Apply the mask to propogate relenace forwards\n",
    "        relevance = initial_out * mask\n",
    "        # loop backwards from output to input layer\n",
    "        for name, actual_module in list(self.net.named_modules())[::-1]:\n",
    "            if len(list(actual_module.children())) == 0:\n",
    "                # if the module is a leaf module, apply LRP\n",
    "                # print(f\"reversing layer {name} which is of type {type(actual_module)}\")\n",
    "                relevance = self._apply_lrp(name, actual_module, relevance.detach())\n",
    "        return relevance\n",
    "    \n",
    "\n",
    "    def _apply_lrp(self, name:str, actual_module:torch.nn.Module, relevance_to_be_propagaed:torch.Tensor):\n",
    "        # Get the activation of the module\n",
    "        layer_activation_values = self.activations[name]\n",
    "        # check datatypes coming through\n",
    "        assert isinstance(layer_activation_values, torch.Tensor)\n",
    "        assert isinstance(actual_module, nn.Module)\n",
    "        assert isinstance(relevance_to_be_propagaed, torch.Tensor)\n",
    "        # Check that the shape of the layer outputs and relevance are the same\n",
    "        if not self.outputs[name].shape == relevance_to_be_propagaed.shape:\n",
    "            print(f\"shapes didn't match for layer {name}\")\n",
    "            relevance_to_be_propagaed = relevance_to_be_propagaed.view(self.outputs[name].shape)\n",
    "        # Get the relevance of the output & apply LRP\n",
    "        relevance = self._reverse_layer_(layer_activation_values, actual_module, relevance_to_be_propagaed)\n",
    "        return relevance\n",
    "    \n",
    "    def _reverse_layer_(self, activations_at_start:torch.Tensor, actual_module:torch.nn.Module, relevance:torch.Tensor, epsilon=1e-9):\n",
    "        # make sure corret data is coming in\n",
    "        assert isinstance(activations_at_start, torch.Tensor), f\"Expected activations_at_start to be a torch.Tensor, got {type(activations_at_start)}\"\n",
    "        assert isinstance(actual_module, nn.Module), f\"Expected actual_module to be an nn.Module, got {type(actual_module)}\"\n",
    "        assert isinstance(relevance, torch.Tensor), f\"Expected relevance to be a torch.Tensor, got {type(relevance)}\"\n",
    "        # print(f\"activations_at_start shape: {activations_at_start.shape}\")\n",
    "        activations_at_start.requires_grad_()\n",
    "        activations_at_start.retain_grad()\n",
    "        # perform a modified forward pass (alpha beta rule applied here apparently)\n",
    "        z = epsilon + actual_module.forward(activations_at_start)\n",
    "        # divide the outputs by the relevance\n",
    "        s = torch.div(relevance, z)\n",
    "        # multiply with weights matrix and perform a backwards pass to get the unit relevance\n",
    "        torch.multiply(z, s.data).sum().backward()\n",
    "        # multiple activations with gradients to get the final relevance\n",
    "        c = activations_at_start * activations_at_start.grad\n",
    "        return c\n",
    "\n",
    "        \n",
    "    def get_activations(self):\n",
    "        return self.activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example Usage\n",
    "# Instantiate and use the wrapper\n",
    "model = SimpleNet()\n",
    "wrapped_model = DiffLrpWrapper(model)\n",
    "target_class = torch.randint(0, 10, (20,1))\n",
    "\n",
    "target_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Forward pass\n",
    "input_tensor = torch.randn(20, 1, 28, 28)\n",
    "output = wrapped_model(input_tensor, target_class)\n",
    "output.sum().backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_layers(model, prefix=\"\"):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Sequential):\n",
    "            # Recursive call to handle nested structures\n",
    "            print(f\"{prefix}{name} (Sequential):\")\n",
    "            print_layers(module, prefix=prefix + \"  \")\n",
    "        else:\n",
    "            # Print layer type\n",
    "            print(f\"{prefix}{name}: {type(module)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in wrapped_model.net.named_modules():\n",
    "    print(name, type(module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(wrapped_model.net.features[0]) is nn.Conv2d:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from captum.attr import LRP\n",
    "\n",
    "atr = LRP(wrapped_model.net)\n",
    "\n",
    "output = wrapped_model(input_tensor)\n",
    "attr = atr.attribute(input_tensor, target=0)\n",
    "loss = attr.sum() - 10\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if one can manually define the gradient computation at run time with this function\n",
    "\n",
    "# Step 1: Define the tensor\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Step 2: Define the function, e.g., f = x^2\n",
    "y = x ** 2\n",
    "\n",
    "# Step 3: Calculate the gradient manually\n",
    "grads = torch.autograd.grad(outputs=y, inputs=x, grad_outputs=torch.tensor([1.0, 1.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "def copy_layer(layer):\n",
    "    # Create a deep copy of the layer\n",
    "    layer_copy = copy.deepcopy(layer)\n",
    "\n",
    "    return layer_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prime = copy_layer(wrapped_model.net.features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_layer(activations_at_start, layer, relevance, epsilon=1e-9):\n",
    "    activations_at_start.requires_grad_()\n",
    "    activations_at_start.retain_grad()\n",
    "    # perform a modified forward pass (alpha beta rule applied here apparently)\n",
    "    z = epsilon + copy_layer(layer).forward(activations_at_start)\n",
    "    # divide the outputs by the relevance\n",
    "    s = torch.div(relevance, z)\n",
    "    # \n",
    "    torch.multiply(z, s.data).sum().backward()\n",
    "    c = activations * activations.grad\n",
    "    return c\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in list(wrapped_model.net.named_modules())[::-1]:\n",
    "            if len(list(module.children())) == 0:\n",
    "                print(f\"name: {name} module: {type(module)}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = wrapped_model.net.classifier[2]\n",
    "print(type(layer))\n",
    "input_tensor = torch.randn(1, 1, 28, 28, requires_grad=True)\n",
    "starting_relevance = wrapped_model.net(input_tensor)\n",
    "activations_at_start = wrapped_model.get_activations()[\"classifier.1\"].requires_grad_()\n",
    "activations_at_start.retain_grad()\n",
    "epsilon = 1e-9\n",
    "# reverse_layer(activations, layer, starting_relevance)\n",
    "z = epsilon + layer.forward(activations_at_start)\n",
    "# divide the outputs by the relevance\n",
    "s = torch.div(starting_relevance, z)\n",
    "# \n",
    "torch.multiply(z, s.detach()).sum().backward()\n",
    "c = activations_at_start * activations_at_start.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minatar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
