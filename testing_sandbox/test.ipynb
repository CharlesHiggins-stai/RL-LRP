{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 20, kernel_size=5),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(320, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 320)  # Flatten the output for the classifier\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class DiffLrpWrapper(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        assert isinstance(net, nn.Module), f\"Expected net to be an instance of nn.Module, got {type(net)}\"\n",
    "        self.net = net\n",
    "        self.activations = {}\n",
    "        self.outputs = {}\n",
    "        self._register_hooks()\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        # Register a forward hook on each module\n",
    "        for name, module in self.net.named_modules():\n",
    "            # Avoid registering hooks on containers\n",
    "            if len(list(module.children())) == 0:\n",
    "                module.register_forward_hook(self._save_activation(name))\n",
    "\n",
    "    def _save_activation(self, name):\n",
    "        # This method returns a hook function\n",
    "        def hook(module, input, output):\n",
    "            self.activations[name] = input[0].detach()\n",
    "            self.outputs[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x, target_class:torch.Tensor):\n",
    "        assert x.shape[0] == target_class.shape[0], f\"Expected x and target_class to have the same batch size, got {x.shape[0]} and {target_class.shape[0]}\"\n",
    "        # Forward pass through the network\n",
    "        initial_out = self.net(x)\n",
    "        # Create a mask that zeros out all elements except for the target class\n",
    "        mask = torch.zeros_like(initial_out)\n",
    "        mask[torch.arange(mask.size(0)), target_class] = 1  # Ensure target_class is either a scalar or has the same batch size as x\n",
    "\n",
    "        # Apply the mask to propogate relenace forwards\n",
    "        relevance = initial_out * mask\n",
    "        # loop backwards from output to input layer\n",
    "        for name, actual_module in list(self.net.named_modules())[::-1]:\n",
    "            if len(list(actual_module.children())) == 0:\n",
    "                # if the module is a leaf module, apply LRP\n",
    "                # print(f\"reversing layer {name} which is of type {type(actual_module)}\")\n",
    "                relevance = self._apply_lrp(name, actual_module, relevance.detach())\n",
    "        return relevance\n",
    "    \n",
    "\n",
    "    def _apply_lrp(self, name:str, actual_module:torch.nn.Module, relevance_to_be_propagaed:torch.Tensor):\n",
    "        # Get the activation of the module\n",
    "        layer_activation_values = self.activations[name]\n",
    "        # check datatypes coming through\n",
    "        assert isinstance(layer_activation_values, torch.Tensor)\n",
    "        assert isinstance(actual_module, nn.Module)\n",
    "        assert isinstance(relevance_to_be_propagaed, torch.Tensor)\n",
    "        # Check that the shape of the layer outputs and relevance are the same\n",
    "        if not self.outputs[name].shape == relevance_to_be_propagaed.shape:\n",
    "            print(f\"shapes didn't match for layer {name}\")\n",
    "            relevance_to_be_propagaed = relevance_to_be_propagaed.view(self.outputs[name].shape)\n",
    "        # Get the relevance of the output & apply LRP\n",
    "        relevance = self._reverse_layer_(layer_activation_values, actual_module, relevance_to_be_propagaed)\n",
    "        return relevance\n",
    "    \n",
    "    def _reverse_layer_(self, activations_at_start:torch.Tensor, actual_module:torch.nn.Module, relevance:torch.Tensor, epsilon=1e-9):\n",
    "        # make sure corret data is coming in\n",
    "        assert isinstance(activations_at_start, torch.Tensor), f\"Expected activations_at_start to be a torch.Tensor, got {type(activations_at_start)}\"\n",
    "        assert isinstance(actual_module, nn.Module), f\"Expected actual_module to be an nn.Module, got {type(actual_module)}\"\n",
    "        assert isinstance(relevance, torch.Tensor), f\"Expected relevance to be a torch.Tensor, got {type(relevance)}\"\n",
    "        # print(f\"activations_at_start shape: {activations_at_start.shape}\")\n",
    "        activations_at_start.requires_grad_()\n",
    "        activations_at_start.retain_grad()\n",
    "        # perform a modified forward pass (alpha beta rule applied here apparently)\n",
    "        z = epsilon + actual_module.forward(activations_at_start)\n",
    "        # divide the outputs by the relevance\n",
    "        s = torch.div(relevance, z)\n",
    "        # multiply with weights matrix and perform a backwards pass to get the unit relevance\n",
    "        torch.multiply(z, s.data).sum().backward()\n",
    "        # multiple activations with gradients to get the final relevance\n",
    "        c = activations_at_start * activations_at_start.grad\n",
    "        return c\n",
    "\n",
    "        \n",
    "    def get_activations(self):\n",
    "        return self.activations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example Usage\n",
    "# Instantiate and use the wrapper\n",
    "model = SimpleNet()\n",
    "wrapped_model = DiffLrpWrapper(model)\n",
    "target_class = torch.randint(0, 10, (20,1))\n",
    "\n",
    "target_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Forward pass\n",
    "input_tensor = torch.randn(20, 1, 28, 28)\n",
    "output = wrapped_model(input_tensor, target_class)\n",
    "output.sum().backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_layers(model, prefix=\"\"):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Sequential):\n",
    "            # Recursive call to handle nested structures\n",
    "            print(f\"{prefix}{name} (Sequential):\")\n",
    "            print_layers(module, prefix=prefix + \"  \")\n",
    "        else:\n",
    "            # Print layer type\n",
    "            print(f\"{prefix}{name}: {type(module)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in wrapped_model.net.named_modules():\n",
    "    print(name, type(module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(wrapped_model.net.features[0]) is nn.Conv2d:\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from captum.attr import LRP\n",
    "\n",
    "atr = LRP(wrapped_model.net)\n",
    "\n",
    "output = wrapped_model(input_tensor)\n",
    "attr = atr.attribute(input_tensor, target=0)\n",
    "loss = attr.sum() - 10\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if one can manually define the gradient computation at run time with this function\n",
    "\n",
    "# Step 1: Define the tensor\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Step 2: Define the function, e.g., f = x^2\n",
    "y = x ** 2\n",
    "\n",
    "# Step 3: Calculate the gradient manually\n",
    "grads = torch.autograd.grad(outputs=y, inputs=x, grad_outputs=torch.tensor([1.0, 1.0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "def copy_layer(layer):\n",
    "    # Create a deep copy of the layer\n",
    "    layer_copy = copy.deepcopy(layer)\n",
    "\n",
    "    return layer_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prime = copy_layer(wrapped_model.net.features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_layer(activations_at_start, layer, relevance, epsilon=1e-9):\n",
    "    activations_at_start.requires_grad_()\n",
    "    activations_at_start.retain_grad()\n",
    "    # perform a modified forward pass (alpha beta rule applied here apparently)\n",
    "    z = epsilon + copy_layer(layer).forward(activations_at_start)\n",
    "    # divide the outputs by the relevance\n",
    "    s = torch.div(relevance, z)\n",
    "    # \n",
    "    torch.multiply(z, s.data).sum().backward()\n",
    "    c = activations * activations.grad\n",
    "    return c\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in list(wrapped_model.net.named_modules())[::-1]:\n",
    "            if len(list(module.children())) == 0:\n",
    "                print(f\"name: {name} module: {type(module)}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = wrapped_model.net.classifier[2]\n",
    "print(type(layer))\n",
    "input_tensor = torch.randn(1, 1, 28, 28, requires_grad=True)\n",
    "starting_relevance = wrapped_model.net(input_tensor)\n",
    "activations_at_start = wrapped_model.get_activations()[\"classifier.1\"].requires_grad_()\n",
    "activations_at_start.retain_grad()\n",
    "epsilon = 1e-9\n",
    "# reverse_layer(activations, layer, starting_relevance)\n",
    "z = epsilon + layer.forward(activations_at_start)\n",
    "# divide the outputs by the relevance\n",
    "s = torch.div(starting_relevance, z)\n",
    "# \n",
    "torch.multiply(z, s.detach()).sum().backward()\n",
    "c = activations_at_start * activations_at_start.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reversing layer relu.layer which is of type <class 'torch.nn.modules.activation.ReLU'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ReLU' object has no attribute 'layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 114\u001b[0m\n\u001b[1;32m    111\u001b[0m target_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Perform LRP\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m R \u001b[38;5;241m=\u001b[39m \u001b[43mlrp_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlrp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(R)\n",
      "Cell \u001b[0;32mIn[9], line 78\u001b[0m, in \u001b[0;36mLRPModel.lrp\u001b[0;34m(self, x, target_class)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(layer\u001b[38;5;241m.\u001b[39mchildren())) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreversing layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which is of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[1;32m     79\u001b[0m         R \u001b[38;5;241m=\u001b[39m lrp_linear(layer, R)\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer\u001b[38;5;241m.\u001b[39mlayer, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mConv2d):\n",
      "File \u001b[0;32m~/miniconda3/envs/minatar/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ReLU' object has no attribute 'layer'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class InputStoringLayer(torch.nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super(InputStoringLayer, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return self.layer(x)\n",
    "\n",
    "def lrp_linear(layer, R, eps=1e-6):\n",
    "    \"\"\"\n",
    "    LRP for a linear layer.\n",
    "    Arguments:\n",
    "        layer: the linear layer (InputStoringLayer wrapping nn.Linear)\n",
    "        R: relevance scores from the previous layer (Tensor)\n",
    "        eps: small value to avoid division by zero (float)\n",
    "    Returns:\n",
    "        relevance scores for the input of this layer (Tensor)\n",
    "    \"\"\"\n",
    "    W = layer.layer.weight\n",
    "    X = layer.input\n",
    "    Z = W @ X.t() + layer.layer.bias[:, None] + eps\n",
    "    S = R / Z\n",
    "    C = W.t() @ S\n",
    "    R_new = X * C.t()\n",
    "    return R_new\n",
    "\n",
    "def lrp_conv2d(layer, R, eps=1e-6):\n",
    "    \"\"\"\n",
    "    LRP for a convolutional layer.\n",
    "    Arguments:\n",
    "        layer: the convolutional layer (InputStoringLayer wrapping nn.Conv2d)\n",
    "        R: relevance scores from the previous layer (Tensor)\n",
    "        eps: small value to avoid division by zero (float)\n",
    "    Returns:\n",
    "        relevance scores for the input of this layer (Tensor)\n",
    "    \"\"\"\n",
    "    W = layer.layer.weight\n",
    "    X = layer.input\n",
    "    Z = F.conv2d(X, W, bias=layer.layer.bias, stride=layer.layer.stride, padding=layer.layer.padding) + eps\n",
    "    S = R / Z\n",
    "    C = F.conv_transpose2d(S, W, stride=layer.layer.stride, padding=layer.layer.padding)\n",
    "    R_new = X * C\n",
    "    return R_new\n",
    "\n",
    "class LRPModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(LRPModel, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def lrp(self, x, target_class):\n",
    "        \"\"\"\n",
    "        Perform LRP on the model.\n",
    "        Arguments:\n",
    "            x: input data (Tensor)\n",
    "            target_class: index of the target class (int)\n",
    "        Returns:\n",
    "            relevance scores for the input (Tensor)\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        output = self.model(x)\n",
    "        \n",
    "        # Initialize relevance for the output layer\n",
    "        R = torch.zeros_like(output)\n",
    "        R[:, target_class] = output[:, target_class]\n",
    "        \n",
    "        # Perform backward pass for LRP\n",
    "        for layer in reversed(list(self.model.children())):\n",
    "                print(f\"reversing layer {layer} which is of type {type(layer)}\")\n",
    "                if isinstance(layer.layer, torch.nn.Linear):\n",
    "                    R = lrp_linear(layer, R)\n",
    "                elif isinstance(layer.layer, torch.nn.Conv2d):\n",
    "                    R = lrp_conv2d(layer, R)\n",
    "                elif isinstance(layer.layer, torch.nn.ReLU):\n",
    "                    R = R * (layer.input > 0).float()\n",
    "        \n",
    "        return R\n",
    "\n",
    "# Define a simple model for demonstration\n",
    "class SimpleCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = InputStoringLayer(torch.nn.Conv2d(1, 10, kernel_size=5))\n",
    "        self.conv2 = InputStoringLayer(torch.nn.Conv2d(10, 20, kernel_size=5))\n",
    "        self.fc1 = InputStoringLayer(torch.nn.Linear(320, 50))\n",
    "        self.fc2 = InputStoringLayer(torch.nn.Linear(50, 10))\n",
    "        self.relu = InputStoringLayer(torch.nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Example usage\n",
    "model = SimpleCNN()\n",
    "lrp_model = LRPModel(model)\n",
    "\n",
    "# Dummy input\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "target_class = 0\n",
    "\n",
    "# Perform LRP\n",
    "R = lrp_model.lrp(x, target_class)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minatar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
