{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from experiments import perform_gradcam, perform_lrp_captum\n",
    "from internal_utils import preprocess_images, condense_to_heatmap, blur_image_batch, add_random_noise_batch, get_data_imagenette, get_teacher_model, get_CIFAR10_dataloader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def visualise_panel_image(image, model, kernel_size_min, kernel_size_max, noise_level_min, noise_level_max, method, label):\n",
    "    \"\"\"Visualise the panel of images for the model.\"\"\"\n",
    "    # Assume the image tensor is already in batch format, if not, unsqueeze it\n",
    "    if image.dim() == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    \n",
    "    original_image = image\n",
    "    # treated images\n",
    "    blurred_small = blur_image_batch(image, kernel_size_min)\n",
    "    blurred_large = blur_image_batch(image, kernel_size_max)\n",
    "    noisy_small = add_random_noise_batch(image, noise_level_min)\n",
    "    noisy_large = add_random_noise_batch(image, noise_level_max)\n",
    "    \n",
    "    # model outputs\n",
    "    original_heatmap = condense_to_heatmap(method(preprocess_images(image), label, model)).detach()\n",
    "    blurred_small_heatmap = condense_to_heatmap(method(preprocess_images(blurred_small), label, model)).detach()\n",
    "    blurred_large_heatmap = condense_to_heatmap(method(preprocess_images(blurred_large),label,  model)).detach()\n",
    "    noisy_small_heatmap = condense_to_heatmap( method(preprocess_images(noisy_small), label, model)).detach()\n",
    "    noisy_large_heatmap = condense_to_heatmap(method(preprocess_images(noisy_large), label, model)).detach()\n",
    "    \n",
    "    # Display images\n",
    "    fig, ax = plt.subplots(2, 5, figsize=(15, 5))\n",
    "    ax[0][0].imshow(original_image.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax[0][0].set_title('Original Image')\n",
    "    ax[0][1].imshow(blurred_small.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax[0][1].set_title('Small Blurred Image')\n",
    "    ax[0][2].imshow(blurred_large.squeeze().permute(1, 2, 0).cpu().numpy())\n",
    "    ax[0][2].set_title('Large Blurred Image')\n",
    "    ax[0][3].imshow(noisy_small.squeeze().detach().permute(1, 2, 0).cpu().numpy())  # Example visualization\n",
    "    ax[0][3].set_title('Small Noisy Image')\n",
    "    ax[0][4].imshow(noisy_large.squeeze().detach().permute(1, 2, 0).cpu().numpy())  # Example visualization\n",
    "    ax[0][4].set_title('Large Noisy Image')\n",
    "    \n",
    "    ax[1][0].imshow(original_heatmap.squeeze(0), cmap='seismic')\n",
    "    ax[1][0].set_title('Original Heatmap')\n",
    "    ax[1][1].imshow(blurred_small_heatmap.squeeze(0), cmap='seismic')\n",
    "    ax[1][1].set_title('Small Blurred Heatmap')\n",
    "    ax[1][2].imshow(blurred_large_heatmap.squeeze(0), cmap='seismic')\n",
    "    ax[1][2].set_title('Large Blurred Heatmap')\n",
    "    ax[1][3].imshow(noisy_small_heatmap.squeeze(0), cmap ='seismic')  # Example visualization\n",
    "    ax[1][3].set_title('Small Noisy Heatmap')\n",
    "    ax[1][4].imshow(noisy_large_heatmap.squeeze(0), cmap ='seismic')  # Example visualization\n",
    "    ax[1][4].set_title('Large Noisy Heatmap')\n",
    "    \n",
    "    for i in ax:\n",
    "        for j in i:\n",
    "            j.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments import WrapperNet, WrapperNetContrastive\n",
    "import torch\n",
    "from internal_utils import update_dictionary_patch\n",
    "from baselines.trainVggBaselineForCIFAR10.vgg import vgg11\n",
    "\n",
    "def get_teacher_model(teacher_checkpoint_path):\n",
    "    checkpoint = torch.load(teacher_checkpoint_path)\n",
    "    # assume teacher model is vgg11 for now\n",
    "    teacher = vgg11()\n",
    "    try: \n",
    "        checkpoint = update_dictionary_patch(checkpoint)\n",
    "        teacher.load_state_dict(checkpoint['new_state_dict'])\n",
    "    except:\n",
    "        print('Incorrect patch specified')\n",
    "    return teacher\n",
    "data = get_CIFAR10_dataloader()\n",
    "input_images, labels = next(iter(data))\n",
    "teacher_model = WrapperNet(get_teacher_model(\"/home/charleshiggins/RL-LRP/baselines/trainVggBaselineForCIFAR10/save_vgg11/checkpoint_299.tar\"), hybrid_loss=True)\n",
    "# define params\n",
    "learner_model = WrapperNet(vgg11(), hybrid_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_images, labels = next(iter(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image, sample_label = input_images[0], labels[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments import perform_lrp_plain\n",
    "visualise_panel_image(sample_image.unsqueeze(0), teacher_model, 3, 15, 0.1, 0.5, perform_lrp_plain, sample_label.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_panel_image(sample_image.unsqueeze(0), learner_model, 3, 15, 0.1, 0.5, perform_lrp_plain, sample_label.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_images = preprocess_images(input_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The target tensor should be: {labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, heatmap = learner_model(pp_images)\n",
    "output_target, heatmap_target = teacher_model(pp_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CosineDistanceLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosineDistanceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # Flatten the images: shape from [b, 1, 28, 28] to [b, 784]\n",
    "        input1_flat = input1.view(input1.size(0), -1)\n",
    "        input2_flat = input2.view(input2.size(0), -1)\n",
    "        \n",
    "        # Compute cosine similarity, then convert to cosine distance\n",
    "        cosine_sim = F.cosine_similarity(input1_flat, input2_flat)\n",
    "        cosine_dist = 1 - cosine_sim\n",
    "        \n",
    "        # Calculate the mean of the cosine distances\n",
    "        loss = cosine_dist.mean()\n",
    "        # loss = F.mse_loss(input1_flat, input2_flat)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "# Define SSIM loss (we'll minimize 1 - SSIM)\n",
    "class SSIMLoss(torch.nn.Module):\n",
    "    def __init__(self, data_range=1.0, size_average=True, channel=3):\n",
    "        super(SSIMLoss, self).__init__()\n",
    "        self.ssim_module = SSIM(data_range=data_range, size_average=size_average, channel=channel)\n",
    "    \n",
    "    def forward(self, img1, img2):\n",
    "        ssim_value = self.ssim_module(img1, img2)\n",
    "        return 1 - ssim_value\n",
    "    \n",
    "def remove_grad_for_all_but_last_layer(learner_model, optimizer, scheduler):\n",
    "    for name, module in learner_model.model.named_modules():\n",
    "        if not isinstance(module, nn.Sequential) \\\n",
    "        and not isinstance(module, WrapperNet) \\\n",
    "        and not len(list(module.children())) > 0 \\\n",
    "            and type(module) not in [nn.ReLU, nn.MaxPool2d, nn.AdaptiveAvgPool2d, nn.LogSoftmax, nn.Dropout]:\n",
    "            # print(name)\n",
    "            # print(module)\n",
    "            # print(\"####################### \\n\")\n",
    "            if \"classifier.6\" not in name:\n",
    "                print(f\"removing grad from: {name}\")\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False\n",
    "            else:\n",
    "                print(f\"Grad will continue for {name}\")\n",
    "    optimizer = torch.optim.SGD(learner_model.parameters(), lr=0.25, momentum=0.9, weight_decay=5e-4)\n",
    "    # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
    "\n",
    "    return learner_model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "optimizer = torch.optim.SGD(learner_model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "cos_loss = CosineDistanceLoss()\n",
    "ssim_loss = SSIMLoss()\n",
    "EPOCHS = 200\n",
    "CHANGE_POINT = 100\n",
    "with torch.no_grad():\n",
    "    _, teacher_heatmap = teacher_model(pp_images)\n",
    "for i in range(0, EPOCHS):\n",
    "    if i <= CHANGE_POINT:\n",
    "        model_out, model_heatmap = learner_model(pp_images, labels)\n",
    "    else:\n",
    "        model_out, model_heatmap = learner_model(pp_images)\n",
    "    optimizer.zero_grad()\n",
    "    loss = ssim_loss(model_heatmap, teacher_heatmap)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(learner_model.parameters(), 1)\n",
    "    optimizer.step()\n",
    "    # if i > CHANGE_POINT:\n",
    "    #     scheduler.step()\n",
    "    correct = model_out.argmax(dim=1).eq(labels).sum().item()\n",
    "    correct_pct = 100 * correct/labels.shape[0] \n",
    "    print(f\"iteration: {i} \\t accuracy: {correct_pct}\\t loss: {loss.float()}\")\n",
    "    if i == CHANGE_POINT:\n",
    "        learner_model, optimizer, scheduler = remove_grad_for_all_but_last_layer(learner_model, optimizer, scheduler)\n",
    "        print(\"REMOVED GRADIENTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_panel_image(two_images[0].unsqueeze(0), learner_model, 3, 15, 0.1, 0.5, perform_lrp_plain, two_labels[0].unsqueeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_lrp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
